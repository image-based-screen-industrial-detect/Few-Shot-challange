
<h1>Evaluation Details</h1>

<p>Submissions will be evaluated based on the mean Average Precision (mAP) metric at an Intersection over Union (IoU) threshold of 0.95 (i.e., mAP@0.95).</p>

<p>Participants must submit their model predictions in the specified JSON format. Each entry should include bounding box coordinates, defect class, and confidence scores for each detected object.</p>

<p>Leaderboard rankings will be determined by the mAP@0.95 score calculated over the test set.</p>
